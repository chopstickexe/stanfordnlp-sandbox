{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まずは動かしてみる（英語・日本語）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "# \n",
    "# 以下、初回のみ実行（30分ほどかかるので注意）\n",
    "# \n",
    "# stanfordnlp.download('en')\n",
    "# stanfordnlp.download('ja')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 英文のtokenizationと依存構造解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\lisa/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\lisa/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\lisa/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\lisa/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\lisa/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\lisa/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', '2', 'nsubj')\n",
      "('like', '0', 'root')\n",
      "('tea', '2', 'obj')\n",
      "('.', '2', 'punct')\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I like tea.\")\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsubj`: 主語になっている名詞句。このrelationを持っている`I`は2番目の語`like`に依存している。\n",
    "\n",
    "`root`: 文の主辞 (head)。このrelationを持っている`like`はこの文の依存構造木のrootになっているため、依存先のIDが0になっている。\n",
    "\n",
    "`obj`: 目的語になっている名詞句。このrelationを持っている`tea`は2番目の語`like`に依存している。\n",
    "\n",
    "`punct`: 句読点。このrelationを持っている`.`は2番目の語`like`に依存している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsubj:pass`: Passive nominal subject 受動態になっている動詞の主語（4というのは、4番目の`born`に依存 (depend) していることを示している）\n",
    "\n",
    "`flat`: Flat multiword expression 複合語の一部であることを示すrelationの一つ。headが無い名詞句（人名や日付など）の一部を構成する語に付与される。なお、複合語に付与される他のrelationにfixedとcompoundがある。\n",
    "\n",
    "`aux:pass`: Auxiliary テンスや事実性などを表現する機能語\n",
    "\n",
    "`case`: Case marking 前置詞や格助詞など。6番目の`Hawaii`に依存している。\n",
    "\n",
    "`obl`: Oblique nominal Subjectでもobjectでもない名詞句。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手元のGPUでは2つのpipelineをいちどにロードするとメモリが不足するため、いったん英語のpipelineを削除してリソースを解放\n",
    "\n",
    "import gc\n",
    "\n",
    "# Pipelineの削除\n",
    "del nlp\n",
    "\n",
    "# Pipelineが利用していたリソースの解放\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 和文のtokenizationと依存構造解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\lisa/stanfordnlp_resources/ja_gsd_models/ja_gsd_tokenizer.pt', 'lang': 'ja', 'shorthand': 'ja_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\lisa/stanfordnlp_resources/ja_gsd_models/ja_gsd_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\lisa/stanfordnlp_resources/ja_gsd_models/ja_gsd.pretrain.pt', 'lang': 'ja', 'shorthand': 'ja_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\lisa/stanfordnlp_resources/ja_gsd_models/ja_gsd_lemmatizer.pt', 'lang': 'ja', 'shorthand': 'ja_gsd', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\lisa/stanfordnlp_resources/ja_gsd_models/ja_gsd_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\lisa/stanfordnlp_resources/ja_gsd_models/ja_gsd.pretrain.pt', 'lang': 'ja', 'shorthand': 'ja_gsd', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "('バラク', '3', 'nmod')\n",
      "('・', '1', 'compound')\n",
      "('オバマ', '7', 'nsubj')\n",
      "('は', '3', 'case')\n",
      "('ハワイ', '7', 'obl')\n",
      "('で', '5', 'case')\n",
      "('生まれ', '0', 'root')\n",
      "('た', '7', 'aux')\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline(lang='ja')\n",
    "doc = nlp(\"バラク・オバマはハワイで生まれた\")\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('バラクオバマ', '5', 'nsubj')\n",
      "('は', '1', 'case')\n",
      "('ハワイ', '5', 'obl')\n",
      "('で', '3', 'case')\n",
      "('生まれ', '0', 'root')\n",
      "('た', '5', 'aux')\n",
      "('。', '5', 'punct')\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"バラクオバマはハワイで生まれた。\")\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('東京', '2', 'compound')\n",
      "('スカイツリー', '6', 'nmod')\n",
      "('へ', '2', 'case')\n",
      "('の', '2', 'case')\n",
      "('行き', '6', 'amod')\n",
      "('方', '8', 'obj')\n",
      "('を', '6', 'case')\n",
      "('教え', '0', 'root')\n",
      "('て', '8', 'mark')\n",
      "('ください', '8', 'aux')\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"東京スカイツリーへの行き方を教えてください\")\n",
    "doc.sentences[0].print_dependencies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
